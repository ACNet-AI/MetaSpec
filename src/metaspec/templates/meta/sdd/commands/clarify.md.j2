---
description: Interactive clarification workflow for toolkit specifications - ask up to 5 targeted questions to resolve ambiguities (inspired by spec-kit)
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Goal

Detect and reduce ambiguity or missing decisions in the toolkit specification through **interactive questioning**. This command runs BEFORE `/metaspec.sdd.plan`.

**Key Philosophy** (inspired by spec-kit):
- Ask ONE question at a time (not a batch report)
- AI recommends the best option for each question
- Maximum 5 questions to avoid user fatigue
- Integrate answers incrementally into spec.md

---

## Execution Flow

### 1. Load toolkit specification

**Read specification**:
```bash
specs/toolkit/{spec_id}-{name}/spec.md
```

**Also load domain dependency**:
```bash
specs/domain/{domain_spec_id}-{domain_name}/spec.md
```

**If missing**: Instruct user to run `/metaspec.sdd.specify` first.

---

### 2. Perform ambiguity scan

**Scan toolkit specification using taxonomy**. Mark each category: **Clear** / **Partial** / **Missing**.

**Taxonomy Categories** (toolkit-specific):

#### **Entity Design Clarity**
- Are all toolkit entity fields defined with types?
- Are required vs optional fields clear?
- Are field descriptions specific enough for implementation?
- Are field validation rules documented?
- Are example values provided?
- Does entity design follow Entity-First principle (3-5 core fields)?

#### **Validation Rules Completeness**
- Are structural validation rules (type, required) defined?
- Are semantic validation rules (cross-field, business logic) specified?
- Are domain-specific validation rules documented with rationale?
- Is error message format specified with examples?
- Are error messages AI-friendly (actionable, with fix suggestions)?
- Is validation performance target specified (e.g., <100ms)?
- Is custom validator registration mechanism defined?

#### **Workflow Definitions**
- Are workflow steps clearly ordered and numbered?
- Are inputs and outputs specified for each step?
- Are CLI commands mapped to workflow steps?
- Are success indicators measurable?
- Are error scenarios documented for each step?
- Are workflow examples provided?
- Is the init → validate → execute flow clearly defined?

#### **CLI Interface Design**
- Are all CLI commands listed with clear purposes?
- Are command arguments and options documented?
- Are command examples provided?
- Are exit codes specified (0=success, 1=failure)?
- Is output format defined (text and JSON)?
- Are error messages for CLI errors specified?
- Is help text format defined?
- Is command naming consistent (init, validate, run)?

#### **Domain Dependency Alignment**
- Are all domain entities referenced in "Dependencies" section?
- Does toolkit spec correctly describe domain entities it will parse/validate?
- Do toolkit validation rules align with domain validation rules?
- Do toolkit operations cover all domain operations?
- Do toolkit error codes align with domain error codes?

#### **Acceptance Criteria Quality**
- Are acceptance criteria measurable and testable?
- Do acceptance criteria cover all user scenarios?
- Are success and failure cases clearly distinguished?
- Are edge cases addressed in acceptance criteria?

#### **Constitution Alignment**
- Does entity design follow Entity-First principle (3-5 fields)?
- Does validator design support extensibility (custom validators)?
- Does spec-first workflow enable users to write specs first?
- Are error messages AI-friendly (clear, actionable)?
- Is progressive enhancement path clear (MVP vs future features)?
- Are domain-specific constraints documented and justified?

#### **Quality Attributes**
- Are performance targets specified (validation <100ms)?
- Are scalability considerations documented (max spec size)?
- Are observability requirements defined (error logging)?
- Are security considerations addressed?

#### **Terminology Consistency**
- Are canonical terms defined and used consistently?
- Is terminology aligned with domain specification?
- Are ambiguous adjectives ("fast", "robust") quantified?

**Build internal coverage map** (do not output unless no questions).

---

### 3. Generate prioritized question queue

**Prioritization algorithm** (spec-kit inspired):
```
Priority = Impact × Uncertainty

Impact scores:
- CRITICAL (4): Blocks toolkit implementation, affects architecture
- HIGH (3): Affects multiple toolkit components, testability
- MEDIUM (2): Affects single toolkit component, clarity
- LOW (1): Style, minor improvement

Uncertainty scores:
- Missing (3): No information
- Partial (2): Incomplete information
- Vague (2): Ambiguous information
- Clear (0): Sufficient information
```

**Question generation rules**:
1. Select top 5 questions by Priority score
2. Ensure category diversity (avoid 3+ questions from same category)
3. **Prioritize domain alignment questions** (toolkit must correctly implement domain spec)
4. Format as multiple-choice (2-5 options) OR short-answer (≤5 words)
5. For each question, AI must analyze and recommend best option

**If no high-priority questions exist**: Report "No critical ambiguities detected" and skip to step 7.

---

### 4. Interactive questioning loop

**Present EXACTLY ONE question at a time.**

#### **For multiple-choice questions**:

**Step 1: Analyze options**
- Evaluate each option against toolkit best practices
- Consider implementation language, architecture patterns, domain alignment
- Determine the most suitable option

**Step 2: Present question with recommendation**

```markdown
**Question {n}/5: {question}**

**Recommended:** Option {X} - {option_text}

**Reasoning:** {1-2 sentences explaining why this is best for toolkit implementation}
- {reason 1}
- {reason 2}

| Option | Description |
|--------|-------------|
| A | {option_a_description} |
| B | {option_b_description} ← Recommended |
| C | {option_c_description} |
| Short | Provide your own answer (≤5 words) |

**Reply**: Option letter (e.g., "B"), "yes"/"recommended" to accept, or custom answer.
```

#### **For short-answer questions**:

```markdown
**Question {n}/5: {question}**

**Suggested:** {suggested_answer}

**Reasoning:** {1-2 sentences explaining suggestion}

**Format**: Short answer (≤5 words)

**Reply**: "yes"/"suggested" to accept, or provide your own answer.
```

#### **Process user answer**:

1. **If user says** "yes", "recommended", "suggested":
   - Use your stated recommendation/suggestion

2. **If user provides option letter** (A/B/C):
   - Validate it matches available options
   - If invalid, ask for clarification (doesn't count as new question)

3. **If user provides custom answer**:
   - Validate length (≤5 words for short-answer)
   - If too long, ask for concise version (doesn't count as new question)

4. **Once answer accepted**:
   - Record in working memory
   - Immediately integrate into spec.md (see step 5)
   - Move to next question

#### **Stop conditions**:

- All 5 questions answered
- User says "done", "stop", "skip" (accept partial completion)
- All critical ambiguities resolved (remaining questions become unnecessary)

**Never reveal future queued questions in advance.**

---

### 5. Integration after EACH accepted answer

**Incremental update approach** (spec-kit inspired):

#### **First answer in session**:

1. **Ensure `## Clarifications` section exists**:
   - If missing, create after `## Overview` or `## Context` section
   
2. **Create today's session subsection**:
   ```markdown
   ### Session {YYYY-MM-DD}
   ```

#### **For every answer**:

1. **Append to Clarifications**:
   ```markdown
   - Q: {question} → A: {answer}
   ```

2. **Apply to appropriate section** (based on question category):

   **Entity Design** → Update `## Entity Definitions` section:
   - Add/modify entity fields with types
   - Add field constraints and validation rules
   - Add examples and rationale
   
   **Validation Rules** → Update `## Validation Requirements` section:
   - Add specific validation rules
   - Define error message format
   - Specify custom validator mechanism
   - Add performance targets
   
   **Workflow** → Update `## Workflow Definitions` section:
   - Add/clarify workflow steps
   - Define inputs/outputs
   - Map CLI commands to steps
   - Add success indicators
   
   **CLI Interface** → Update `## CLI Commands` section:
   - Add command details
   - Define arguments and options
   - Specify exit codes
   - Add command examples
   
   **Domain Dependency** → Update `## Dependencies` section:
   - Reference domain entities
   - Clarify how toolkit implements domain spec
   - Ensure alignment with domain validation rules
   
   **Acceptance Criteria** → Update `## Acceptance Criteria` section:
   - Add measurable criteria
   - Define success/failure cases
   - Add edge case coverage
   
   **Constitution** → Update relevant sections:
   - Adjust entity design to follow principles
   - Add custom validator mechanism
   - Clarify progressive enhancement path
   - Add principle justifications
   
   **Quality Attributes** → Update `## Quality Criteria` section:
   - Add performance targets
   - Add scalability limits
   - Add observability requirements
   
   **Terminology** → Normalize across all sections:
   - Replace old term with canonical term
   - Ensure alignment with domain specification
   - Add glossary entry if needed

3. **Remove contradictions**:
   - If answer invalidates earlier statement, replace (don't duplicate)
   - Remove vague placeholders now clarified

4. **Save spec.md immediately** (atomic write after each integration)

5. **Preserve formatting**:
   - Maintain heading hierarchy
   - Don't reorder unrelated sections
   - Keep consistent markdown style

---

### 6. Validation (after EACH write)

**Check**:
- ✅ Clarifications section has one bullet per answer
- ✅ Updated section contains no lingering vague terms
- ✅ No contradictory statements remain
- ✅ Markdown structure valid
- ✅ Terminology consistent across sections
- ✅ Domain alignment maintained (toolkit spec references domain spec correctly)

**If validation fails**: Fix immediately before next question.

---

### 7. Report completion

**After questioning loop ends**:

```markdown
## ✅ Clarification Session Complete

**Summary**:
- Questions asked & answered: {n}/5
- Toolkit specification updated: specs/toolkit/{spec_id}-{name}/spec.md
- Sections touched: {list_of_sections}

**Coverage Status**:

| Category | Before | After | Status |
|----------|--------|-------|--------|
| Entity Design | Partial | Clear | ✅ Resolved |
| Validation Rules | Missing | Clear | ✅ Resolved |
| Workflow | Clear | Clear | ✅ Already sufficient |
| CLI Interface | Partial | Clear | ✅ Resolved |
| Domain Dependency | Missing | Clear | ✅ Resolved |
| Acceptance Criteria | Partial | Partial | ⚠️ Deferred (low impact) |
| Constitution | Clear | Clear | ✅ Already sufficient |
| Quality Attributes | Partial | Clear | ✅ Resolved |
| Terminology | Clear | Clear | ✅ Already sufficient |

**Impact**:
- Resolved: {n} categories (was Partial/Missing, now Clear)
- Deferred: {n} categories (low impact, better for planning phase)
- Outstanding: {n} categories (still Partial/Missing but within question limit)

**Domain Alignment Check**:
{if domain_dependency_resolved}
  ✅ Toolkit spec correctly references domain specification
  ✅ Validation rules aligned with domain requirements
  ✅ All domain entities addressed
{else}
  ⚠️ Domain alignment still needs attention:
  - {alignment_issue_1}
  - {alignment_issue_2}
{endif}

**Recommendation**:
{if no_outstanding_critical}
  ✅ Ready to proceed to `/metaspec.sdd.plan`
{else}
  ⚠️ Consider running `/metaspec.sdd.clarify` again to address:
  - {outstanding_category_1}
  - {outstanding_category_2}
  
  OR proceed to `/metaspec.sdd.plan` (accept minor ambiguities)
{endif}

**Next Command**: `/metaspec.sdd.plan`
```

---

## Special Cases

### **No ambiguities found**:

```markdown
## ✅ No Critical Ambiguities Detected

**Coverage Summary**:
All categories marked **Clear** - toolkit specification is well-defined.

| Category | Status |
|----------|--------|
| Entity Design | ✅ Clear |
| Validation Rules | ✅ Clear |
| Workflow | ✅ Clear |
| CLI Interface | ✅ Clear |
| Domain Dependency | ✅ Clear |
| Acceptance Criteria | ✅ Clear |
| Constitution | ✅ Clear |
| Quality Attributes | ✅ Clear |
| Terminology | ✅ Clear |

**Domain Alignment**: ✅ Toolkit spec correctly implements domain specification

**Recommendation**: Proceed to `/metaspec.sdd.plan`
```

### **User skips clarification**:

Accept early termination:
```markdown
⚠️ Clarification skipped by user after {n} questions.

**Warning**: Proceeding with unresolved ambiguities increases downstream rework risk during toolkit implementation.

**Deferred categories**:
- {category_1}: {brief_issue}
- {category_2}: {brief_issue}

You can:
1. Run `/metaspec.sdd.clarify` again later
2. Proceed to `/metaspec.sdd.plan` and address during planning

**Next Command**: `/metaspec.sdd.plan` (proceed with caution)
```

---

## Operating Principles

### **Token Efficiency** (spec-kit inspired):

- **Progressive disclosure**: Load specs once, build internal model
- **Minimal output**: Only show current question, not full queue
- **Structured answers**: Multiple-choice or short-answer only
- **Incremental writes**: Update spec after each answer (avoid large final merge)

### **User Experience**:

- **One question at a time**: Reduce cognitive load
- **AI recommendations**: Lower decision cost
- **Immediate feedback**: See spec update after each answer
- **Flexible termination**: Accept partial completion

### **Quality Assurance**:

- **Prioritization**: Focus on high-impact ambiguities first
- **Category diversity**: Avoid question clustering
- **Domain alignment**: Prioritize questions that ensure toolkit implements domain spec correctly
- **Validation**: Check consistency after each integration
- **Deterministic**: Same spec state → same priority order

### **Toolkit-Specific Considerations**:

- **Domain dependency is critical**: Always check domain spec alignment
- **Implementation focus**: Questions should help implementation, not just documentation
- **Constitution enforcement**: Toolkit must follow toolkit-specific principles (Entity-First, Validator Extensibility, etc.)

---

## Context

{ARGS}
